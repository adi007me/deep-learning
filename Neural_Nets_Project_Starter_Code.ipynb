{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread, imresize #Aditya\n",
    "import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "import random\n",
    "from skimage import transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot n images using subplots\n",
    "def plot_image(images, captions=None, cmap=None ):\n",
    "    f, axes = plt.subplots(1, len(images), sharey=True)\n",
    "    f.set_figwidth(15)\n",
    "    for ax,image in zip(axes, images):\n",
    "        ax.imshow(image, cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_doc = np.random.permutation(open('/notebooks/storage/Final_data/Collated_training/train.csv').readlines())\n",
    "# val_doc = np.random.permutation(open('/notebooks/storage/Final_data/Collated_training/val.csv').readlines())\n",
    "\n",
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "\n",
    "# For local\n",
    "# train_doc = np.random.permutation(open('Project_data/Project_data/train.csv').readlines())\n",
    "# val_doc = np.random.permutation(open('Project_data/Project_data/val.csv').readlines())\n",
    "\n",
    "batch_size = 3 #experiment with the batch size\n",
    "img_size = 100 #update\n",
    "interval = np.arange(10,26,3)\n",
    "frame_count=30\n",
    "\n",
    "learning_rate = 0.0002\n",
    "NUM_CLASSES = 5\n",
    "filtersize = (2, 2, 2)\n",
    "dense_neurons = 128\n",
    "dropout = 0.5\n",
    "input_shape = (len(interval),img_size,img_size,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    \"\"\"Normalize the image by dividing by 255\"\"\"\n",
    "    image = image.astype('float32')\n",
    "    \n",
    "    return image/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(image, deg):\n",
    "    \"\"\"Rotate the image by deg\"\"\"\n",
    "    shift_x, shift_y = image.shape[0]/2, image.shape[1]/2\n",
    "\n",
    "    # translation by certain units\n",
    "    matrix_to_topleft = transform.SimilarityTransform(translation=[-shift_x, -shift_y])\n",
    "    matrix_to_center = transform.SimilarityTransform(translation=[shift_x, shift_y])\n",
    "\n",
    "    # rotation\n",
    "    rot_transforms =  transform.AffineTransform(rotation=np.deg2rad(deg))\n",
    "    rot_matrix = matrix_to_topleft + rot_transforms + matrix_to_center\n",
    "    rot_image = transform.warp(image, rot_matrix)\n",
    "    return rot_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image):\n",
    "    \"\"\"Crop the image by 50 pixels if its height is more than 200 pixels\"\"\"\n",
    "    w, h, _ = image.shape\n",
    "    crop = 0\n",
    "\n",
    "    if (h > 200):\n",
    "        crop = 50\n",
    "        image[:crop, :] = 0\n",
    "        image[-1:-crop, :] = 0\n",
    "\n",
    "    image = image[crop:h - crop, crop:w-crop, : ]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_exposure(image, gamma):\n",
    "    \"\"\"Adjust brightness\"\"\"\n",
    "    return exposure.adjust_gamma(image, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, size = img_size):\n",
    "    \"\"\"Resize the image\"\"\"\n",
    "    return imresize(image, size=(size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(sequences, labels):\n",
    "    \"\"\"Randomize the augmented and original image sets before yeilding\"\"\"\n",
    "    \n",
    "    indices = np.arange(labels.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    sequences = sequences[indices]\n",
    "    labels = labels[indices]\n",
    "    \n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(source_path, folder_list, batch_size, ablation_size = 0, augmentation=False):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    img_idx = interval#update #Aditya  #create a list of image numbers you want to use for a particular video\n",
    "    \n",
    "    if ablation_size is not 0:\n",
    "        folder_list = folder_list[:ablation_size]\n",
    "    \n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(t)//batch_size #Aditya # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            x = len(img_idx)\n",
    "            y = img_size\n",
    "            z = img_size\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_data_aug = np.zeros((batch_size,x,y,z,3))\n",
    "            batch_data_rot = np.zeros((batch_size,x,y,z,3))\n",
    "                        \n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            gamma = np.random.uniform(low=0.2, high=0.9)\n",
    "            rotation_angle = random.choice([-10,10])\n",
    "            \n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                prev_image = np.zeros((y,z,3))\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    image = resize(crop(image))\n",
    "                                \n",
    "                    #add brightness and rotation\n",
    "                    if augmentation:\n",
    "                        image_aug = adjust_exposure(image, gamma=gamma)\n",
    "                        rot_image = rotate(image, rotation_angle)\n",
    "                                        \n",
    "                    batch_data[folder,idx,:,:,0] = normalize(image[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = normalize(image[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = normalize(image[:,:,2])\n",
    "                \n",
    "                    if augmentation:                        \n",
    "                        batch_data_aug[folder,idx,:,:,0] = normalize(image_aug[:,:,0])\n",
    "                        batch_data_aug[folder,idx,:,:,1] = normalize(image_aug[:,:,1])\n",
    "                        batch_data_aug[folder,idx,:,:,2] = normalize(image_aug[:,:,2])\n",
    "\n",
    "                        batch_data_rot[folder,idx,:,:,0] = normalize(rot_image[:,:,0])\n",
    "                        batch_data_rot[folder,idx,:,:,1] = normalize(rot_image[:,:,1])\n",
    "                        batch_data_rot[folder,idx,:,:,2] = normalize(rot_image[:,:,2])\n",
    "                    \n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                \n",
    "            batch_data1, batch_labels1 = randomize(np.concatenate([batch_data, batch_data_aug, batch_data_rot]), np.concatenate([batch_labels, batch_labels]))\n",
    "                \n",
    "            if augmentation:\n",
    "                batch_data, batch_labels = randomize(np.concatenate([batch_data, batch_data_aug, batch_data_rot]), np.concatenate([batch_labels, batch_labels]))\n",
    "                yield batch_data, batch_labels\n",
    "            else:\n",
    "                yield batch_data, batch_labels\n",
    "\n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        remainder = len(t) % batch_size\n",
    "        if(remainder != 0):\n",
    "            x = len(img_idx)\n",
    "            y = img_size\n",
    "            z = img_size\n",
    "            batch_data = np.zeros((remainder,x,y,z,3))\n",
    "            batch_data_aug = np.zeros((remainder,x,y,z,3))\n",
    "            \n",
    "            batch_labels = np.zeros((remainder, 5))\n",
    "            \n",
    "            gamma = np.random.uniform(low=0.3, high=0.9)\n",
    "            rotation_angle = random.choice([-10,10])\n",
    "            t = t[-remainder:]\n",
    "            for folder in range(remainder): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder].strip().split(';')[0]+'/'+ imgs[item]).astype(np.float32)\n",
    "                   \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    image = resize(crop(image))\n",
    "                    \n",
    "                    #add brightness and rotation\n",
    "                    if augmentation:\n",
    "                        image_aug = adjust_exposure(image, gamma=gamma)\n",
    "                        rot_image = rotate(image, rotation_angle)\n",
    "                                        \n",
    "                    batch_data[folder,idx,:,:,0] = normalize(image[:,:,0])\n",
    "                    batch_data[folder,idx,:,:,1] = normalize(image[:,:,1])\n",
    "                    batch_data[folder,idx,:,:,2] = normalize(image[:,:,2])\n",
    "                \n",
    "                    if augmentation:                        \n",
    "                        batch_data_aug[folder,idx,:,:,0] = normalize(image_aug[:,:,0])\n",
    "                        batch_data_aug[folder,idx,:,:,1] = normalize(image_aug[:,:,1])\n",
    "                        batch_data_aug[folder,idx,:,:,2] = normalize(image_aug[:,:,2])\n",
    "\n",
    "                        batch_data_rot[folder,idx,:,:,0] = normalize(rot_image[:,:,0])\n",
    "                        batch_data_rot[folder,idx,:,:,1] = normalize(rot_image[:,:,1])\n",
    "                        batch_data_rot[folder,idx,:,:,2] = normalize(rot_image[:,:,2])\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder].strip().split(';')[2])] = 1\n",
    "            \n",
    "            if augmentation:\n",
    "                batch_data, batch_labels = randomize(np.concatenate([batch_data, batch_data_aug, batch_data_rot]), np.concatenate([batch_labels, batch_labels]))\n",
    "                yield batch_data, batch_labels\n",
    "            else:\n",
    "                yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to plot the images\n",
    "\n",
    "# train_path = 'Project_data/train'\n",
    "# test_images_generator = generator(train_path, train_doc, batch_size, ablation_size=10, augmentation=False)\n",
    "\n",
    "# test_images = []\n",
    "# counter = 0\n",
    "# for test_img in test_images_generator:\n",
    "#     for vdo,label in zip(test_img[0],test_img[1]):\n",
    "# #         print(vdo.shape)\n",
    "# #         print(label)\n",
    "#         plot_image(vdo[:5],cmap='gray')\n",
    "# #         plot_image(vdo[5:10])\n",
    "# #         plot_image(vdo[12:29])\n",
    "# #     break;\n",
    "    \n",
    "#     counter = counter + 1\n",
    "#     if counter is 5:\n",
    "#         break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 50\n"
     ]
    }
   ],
   "source": [
    "# curr_dt_time = datetime.datetime.now()\n",
    "# train_path = '/notebooks/storage/Final_data/Collated_training/train'\n",
    "# val_path = '/notebooks/storage/Final_data/Collated_training/val'\n",
    "# num_train_sequences = len(train_doc)\n",
    "# print('# training sequences =', num_train_sequences)\n",
    "# num_val_sequences = len(val_doc)\n",
    "# print('# validation sequences =', num_val_sequences)\n",
    "# num_epochs = # choose the number of epochs\n",
    "# print ('# epochs =', num_epochs)\n",
    "\n",
    "curr_dt_time = datetime.datetime.now()\n",
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 50 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:72: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:515: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4048: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:131: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3368: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, LSTM, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation, Dropout\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras import optimizers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv3D(16, filtersize, padding='same',\n",
    "         input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(1, 3, 3)))\n",
    "\n",
    "model.add(Conv3D(32, filtersize, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling3D(pool_size=(1, 3, 3)))\n",
    "\n",
    "# model.add(Conv3D(64, filtersize, padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# model.add(Conv3D(128, filtersize, padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(dense_neurons,activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(dense_neurons,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(NUM_CLASSES,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2),\n",
    "#     activation='relu', padding='same'), input_shape=input_shape))\n",
    "# model.add(TimeDistributed(Conv2D(32, (3,3),\n",
    "#     kernel_initializer=\"he_normal\", activation='relu')))\n",
    "# model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "# model.add(TimeDistributed(Conv2D(64, (3,3),\n",
    "#     padding='same', activation='relu')))\n",
    "# model.add(TimeDistributed(Conv2D(64, (3,3),\n",
    "#     padding='same', activation='relu')))\n",
    "# model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "# model.add(TimeDistributed(Conv2D(128, (3,3),\n",
    "#     padding='same', activation='relu')))\n",
    "# model.add(TimeDistributed(Conv2D(128, (3,3),\n",
    "#     padding='same', activation='relu')))\n",
    "# model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "# model.add(TimeDistributed(Conv2D(256, (3,3),\n",
    "#     padding='same', activation='relu')))\n",
    "# model.add(TimeDistributed(Conv2D(256, (3,3),\n",
    "#     padding='same', activation='relu')))\n",
    "# model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "# model.add(TimeDistributed(Conv2D(512, (3,3),\n",
    "#     padding='same', activation='relu')))\n",
    "# model.add(TimeDistributed(Conv2D(512, (3,3),\n",
    "#     padding='same', activation='relu')))\n",
    "# model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "# model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(LSTM(256, return_sequences=False, dropout=0.5))\n",
    "# model.add(Dense(NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:782: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3218: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 6, 100, 100, 16)   400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6, 100, 100, 16)   0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6, 100, 100, 16)   64        \n",
      "_________________________________________________________________\n",
      "max_pooling3d_1 (MaxPooling3 (None, 6, 33, 33, 16)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_2 (Conv3D)            (None, 6, 33, 33, 32)     4128      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 6, 33, 33, 32)     0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 6, 33, 33, 32)     128       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_2 (MaxPooling3 (None, 6, 11, 11, 32)     0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23232)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               2973824   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,979,701\n",
      "Trainable params: 2,979,349\n",
      "Non-trainable params: 352\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser = optimizers.Adam(lr=learning_rate) #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2,verbose=1,\n",
    "                              patience=5, min_lr=0.0001)# write the REducelronplateau code here\n",
    "\n",
    "ES = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.001,\n",
    "    patience=20,\n",
    "    mode=\"auto\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, LR, ES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try ablation\n",
    "\n",
    "# train_generator = generator(train_path, train_doc, batch_size, ablation_size=100)\n",
    "# val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "# model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=5, verbose=1, \n",
    "#                     callbacks=callbacks_list, validation_data=val_generator, \n",
    "#                     validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Source path =  Project_data/val ; batch size = 3\n",
      "Source path =  Project_data/train ; batch size = 3\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/221 [============================>.] - ETA: 0s - loss: 1.6625 - categorical_accuracy: 0.3545"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:84: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 79s 356ms/step - loss: 1.6693 - categorical_accuracy: 0.3529 - val_loss: 1.2567 - val_categorical_accuracy: 0.5700\n",
      "\n",
      "Epoch 00001: saving model to model_init_2020-12-2511_49_09.401932/model-00001-1.66929-0.35294-1.25675-0.57000.h5\n",
      "Epoch 2/50\n",
      "221/221 [==============================] - 70s 319ms/step - loss: 1.2910 - categorical_accuracy: 0.5000 - val_loss: 1.1652 - val_categorical_accuracy: 0.6100\n",
      "\n",
      "Epoch 00002: saving model to model_init_2020-12-2511_49_09.401932/model-00002-1.29103-0.50000-1.16520-0.61000.h5\n",
      "Epoch 3/50\n",
      "221/221 [==============================] - 79s 358ms/step - loss: 1.1503 - categorical_accuracy: 0.5543 - val_loss: 1.0051 - val_categorical_accuracy: 0.6600\n",
      "\n",
      "Epoch 00003: saving model to model_init_2020-12-2511_49_09.401932/model-00003-1.15027-0.55430-1.00507-0.66000.h5\n",
      "Epoch 4/50\n",
      "221/221 [==============================] - 73s 331ms/step - loss: 1.1343 - categorical_accuracy: 0.5920 - val_loss: 1.0131 - val_categorical_accuracy: 0.5300\n",
      "\n",
      "Epoch 00004: saving model to model_init_2020-12-2511_49_09.401932/model-00004-1.13427-0.59201-1.01305-0.53000.h5\n",
      "Epoch 5/50\n",
      "221/221 [==============================] - 73s 331ms/step - loss: 1.0284 - categorical_accuracy: 0.6222 - val_loss: 0.8189 - val_categorical_accuracy: 0.7000\n",
      "\n",
      "Epoch 00005: saving model to model_init_2020-12-2511_49_09.401932/model-00005-1.02839-0.62217-0.81894-0.70000.h5\n",
      "Epoch 6/50\n",
      "221/221 [==============================] - 68s 309ms/step - loss: 0.9255 - categorical_accuracy: 0.6584 - val_loss: 0.8710 - val_categorical_accuracy: 0.6900\n",
      "\n",
      "Epoch 00006: saving model to model_init_2020-12-2511_49_09.401932/model-00006-0.92547-0.65837-0.87103-0.69000.h5\n",
      "Epoch 7/50\n",
      "221/221 [==============================] - 69s 314ms/step - loss: 0.8680 - categorical_accuracy: 0.6682 - val_loss: 0.9185 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00007: saving model to model_init_2020-12-2511_49_09.401932/model-00007-0.86796-0.66817-0.91847-0.75000.h5\n",
      "Epoch 8/50\n",
      "221/221 [==============================] - 77s 350ms/step - loss: 0.7608 - categorical_accuracy: 0.7270 - val_loss: 0.7759 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00008: saving model to model_init_2020-12-2511_49_09.401932/model-00008-0.76078-0.72700-0.77587-0.72000.h5\n",
      "Epoch 9/50\n",
      "221/221 [==============================] - 75s 342ms/step - loss: 0.7794 - categorical_accuracy: 0.7142 - val_loss: 0.7501 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00009: saving model to model_init_2020-12-2511_49_09.401932/model-00009-0.77945-0.71418-0.75010-0.72000.h5\n",
      "Epoch 10/50\n",
      "221/221 [==============================] - 77s 347ms/step - loss: 0.8268 - categorical_accuracy: 0.7036 - val_loss: 0.7738 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00010: saving model to model_init_2020-12-2511_49_09.401932/model-00010-0.82680-0.70362-0.77381-0.75000.h5\n",
      "Epoch 11/50\n",
      "221/221 [==============================] - 82s 370ms/step - loss: 0.8214 - categorical_accuracy: 0.7051 - val_loss: 0.6552 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00011: saving model to model_init_2020-12-2511_49_09.401932/model-00011-0.82135-0.70513-0.65516-0.74000.h5\n",
      "Epoch 12/50\n",
      "221/221 [==============================] - 70s 317ms/step - loss: 0.6853 - categorical_accuracy: 0.7609 - val_loss: 0.6148 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00012: saving model to model_init_2020-12-2511_49_09.401932/model-00012-0.68531-0.76094-0.61477-0.74000.h5\n",
      "Epoch 13/50\n",
      "221/221 [==============================] - 71s 323ms/step - loss: 0.8178 - categorical_accuracy: 0.7081 - val_loss: 0.6537 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00013: saving model to model_init_2020-12-2511_49_09.401932/model-00013-0.81782-0.70814-0.65374-0.73000.h5\n",
      "Epoch 14/50\n",
      "221/221 [==============================] - 76s 344ms/step - loss: 0.7071 - categorical_accuracy: 0.7557 - val_loss: 0.6685 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00014: saving model to model_init_2020-12-2511_49_09.401932/model-00014-0.70710-0.75566-0.66847-0.73000.h5\n",
      "Epoch 15/50\n",
      "221/221 [==============================] - 78s 354ms/step - loss: 0.5884 - categorical_accuracy: 0.7934 - val_loss: 0.7412 - val_categorical_accuracy: 0.6700\n",
      "\n",
      "Epoch 00015: saving model to model_init_2020-12-2511_49_09.401932/model-00015-0.58841-0.79336-0.74116-0.67000.h5\n",
      "Epoch 16/50\n",
      "221/221 [==============================] - 71s 323ms/step - loss: 0.9045 - categorical_accuracy: 0.6561 - val_loss: 0.7774 - val_categorical_accuracy: 0.7200\n",
      "\n",
      "Epoch 00016: saving model to model_init_2020-12-2511_49_09.401932/model-00016-0.90453-0.65611-0.77737-0.72000.h5\n",
      "Epoch 17/50\n",
      "221/221 [==============================] - 71s 321ms/step - loss: 0.7413 - categorical_accuracy: 0.7308 - val_loss: 0.9018 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00017: saving model to model_init_2020-12-2511_49_09.401932/model-00017-0.74130-0.73077-0.90184-0.74000.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "Epoch 18/50\n",
      "221/221 [==============================] - 72s 327ms/step - loss: 0.6395 - categorical_accuracy: 0.7828 - val_loss: 0.6497 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00018: saving model to model_init_2020-12-2511_49_09.401932/model-00018-0.63953-0.78281-0.64966-0.78000.h5\n",
      "Epoch 19/50\n",
      "221/221 [==============================] - 73s 332ms/step - loss: 0.5333 - categorical_accuracy: 0.8356 - val_loss: 0.6274 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00019: saving model to model_init_2020-12-2511_49_09.401932/model-00019-0.53329-0.83560-0.62745-0.78000.h5\n",
      "Epoch 20/50\n",
      "221/221 [==============================] - 70s 318ms/step - loss: 0.5701 - categorical_accuracy: 0.7964 - val_loss: 0.6002 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00020: saving model to model_init_2020-12-2511_49_09.401932/model-00020-0.57013-0.79638-0.60016-0.79000.h5\n",
      "Epoch 21/50\n",
      "221/221 [==============================] - 71s 319ms/step - loss: 0.5519 - categorical_accuracy: 0.8137 - val_loss: 0.6732 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00021: saving model to model_init_2020-12-2511_49_09.401932/model-00021-0.55189-0.81373-0.67323-0.75000.h5\n",
      "Epoch 22/50\n",
      "221/221 [==============================] - 72s 326ms/step - loss: 0.5922 - categorical_accuracy: 0.7971 - val_loss: 0.6473 - val_categorical_accuracy: 0.7300\n",
      "\n",
      "Epoch 00022: saving model to model_init_2020-12-2511_49_09.401932/model-00022-0.59223-0.79713-0.64734-0.73000.h5\n",
      "Epoch 23/50\n",
      "221/221 [==============================] - 80s 362ms/step - loss: 0.5195 - categorical_accuracy: 0.8371 - val_loss: 0.6057 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00023: saving model to model_init_2020-12-2511_49_09.401932/model-00023-0.51947-0.83710-0.60572-0.77000.h5\n",
      "Epoch 24/50\n",
      "221/221 [==============================] - 74s 336ms/step - loss: 0.4881 - categorical_accuracy: 0.8439 - val_loss: 0.5920 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00024: saving model to model_init_2020-12-2511_49_09.401932/model-00024-0.48811-0.84389-0.59200-0.77000.h5\n",
      "Epoch 25/50\n",
      "221/221 [==============================] - 74s 336ms/step - loss: 0.4820 - categorical_accuracy: 0.8363 - val_loss: 0.6515 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00025: saving model to model_init_2020-12-2511_49_09.401932/model-00025-0.48200-0.83635-0.65149-0.74000.h5\n",
      "Epoch 26/50\n",
      "221/221 [==============================] - 75s 338ms/step - loss: 0.5048 - categorical_accuracy: 0.8454 - val_loss: 0.6160 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00026: saving model to model_init_2020-12-2511_49_09.401932/model-00026-0.50481-0.84540-0.61595-0.77000.h5\n",
      "Epoch 27/50\n",
      "221/221 [==============================] - 77s 348ms/step - loss: 0.5738 - categorical_accuracy: 0.8183 - val_loss: 0.6599 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00027: saving model to model_init_2020-12-2511_49_09.401932/model-00027-0.57382-0.81825-0.65987-0.78000.h5\n",
      "Epoch 28/50\n",
      "221/221 [==============================] - 75s 340ms/step - loss: 0.5546 - categorical_accuracy: 0.8137 - val_loss: 0.6074 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00028: saving model to model_init_2020-12-2511_49_09.401932/model-00028-0.55459-0.81373-0.60738-0.77000.h5\n",
      "Epoch 29/50\n",
      "221/221 [==============================] - 73s 332ms/step - loss: 0.4880 - categorical_accuracy: 0.8318 - val_loss: 0.5862 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00029: saving model to model_init_2020-12-2511_49_09.401932/model-00029-0.48804-0.83183-0.58621-0.83000.h5\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 67s 305ms/step - loss: 0.5026 - categorical_accuracy: 0.8356 - val_loss: 0.6469 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00030: saving model to model_init_2020-12-2511_49_09.401932/model-00030-0.50263-0.83560-0.64687-0.77000.h5\n",
      "Epoch 31/50\n",
      "221/221 [==============================] - 69s 312ms/step - loss: 0.5408 - categorical_accuracy: 0.8175 - val_loss: 0.6347 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00031: saving model to model_init_2020-12-2511_49_09.401932/model-00031-0.54080-0.81750-0.63468-0.80000.h5\n",
      "Epoch 32/50\n",
      "221/221 [==============================] - 73s 330ms/step - loss: 0.5219 - categorical_accuracy: 0.8333 - val_loss: 0.6444 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00032: saving model to model_init_2020-12-2511_49_09.401932/model-00032-0.52194-0.83333-0.64443-0.80000.h5\n",
      "Epoch 33/50\n",
      "221/221 [==============================] - 68s 307ms/step - loss: 0.5074 - categorical_accuracy: 0.8341 - val_loss: 0.5980 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00033: saving model to model_init_2020-12-2511_49_09.401932/model-00033-0.50736-0.83409-0.59798-0.81000.h5\n",
      "Epoch 34/50\n",
      "221/221 [==============================] - 67s 304ms/step - loss: 0.4986 - categorical_accuracy: 0.8318 - val_loss: 0.6204 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00034: saving model to model_init_2020-12-2511_49_09.401932/model-00034-0.49859-0.83183-0.62045-0.80000.h5\n",
      "Epoch 35/50\n",
      "221/221 [==============================] - 69s 314ms/step - loss: 0.4537 - categorical_accuracy: 0.8499 - val_loss: 0.5943 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00035: saving model to model_init_2020-12-2511_49_09.401932/model-00035-0.45370-0.84992-0.59431-0.79000.h5\n",
      "Epoch 36/50\n",
      "221/221 [==============================] - 70s 315ms/step - loss: 0.4787 - categorical_accuracy: 0.8379 - val_loss: 0.6111 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00036: saving model to model_init_2020-12-2511_49_09.401932/model-00036-0.47873-0.83786-0.61113-0.78000.h5\n",
      "Epoch 37/50\n",
      "221/221 [==============================] - 68s 309ms/step - loss: 0.4862 - categorical_accuracy: 0.8265 - val_loss: 0.5882 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00037: saving model to model_init_2020-12-2511_49_09.401932/model-00037-0.48622-0.82655-0.58824-0.77000.h5\n",
      "Epoch 38/50\n",
      "221/221 [==============================] - 68s 306ms/step - loss: 0.4555 - categorical_accuracy: 0.8544 - val_loss: 0.6040 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00038: saving model to model_init_2020-12-2511_49_09.401932/model-00038-0.45551-0.85445-0.60399-0.74000.h5\n",
      "Epoch 39/50\n",
      "221/221 [==============================] - 67s 305ms/step - loss: 0.4504 - categorical_accuracy: 0.8635 - val_loss: 0.6223 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00039: saving model to model_init_2020-12-2511_49_09.401932/model-00039-0.45036-0.86350-0.62227-0.77000.h5\n",
      "Epoch 40/50\n",
      "221/221 [==============================] - 69s 312ms/step - loss: 0.4479 - categorical_accuracy: 0.8446 - val_loss: 0.6500 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00040: saving model to model_init_2020-12-2511_49_09.401932/model-00040-0.44786-0.84465-0.65004-0.79000.h5\n",
      "Epoch 41/50\n",
      "221/221 [==============================] - 68s 308ms/step - loss: 0.4550 - categorical_accuracy: 0.8575 - val_loss: 0.6087 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00041: saving model to model_init_2020-12-2511_49_09.401932/model-00041-0.45495-0.85747-0.60872-0.77000.h5\n",
      "Epoch 42/50\n",
      "221/221 [==============================] - 68s 309ms/step - loss: 0.5291 - categorical_accuracy: 0.8356 - val_loss: 0.6587 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00042: saving model to model_init_2020-12-2511_49_09.401932/model-00042-0.52906-0.83560-0.65873-0.77000.h5\n",
      "Epoch 43/50\n",
      "221/221 [==============================] - 70s 318ms/step - loss: 0.4558 - categorical_accuracy: 0.8514 - val_loss: 0.5765 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00043: saving model to model_init_2020-12-2511_49_09.401932/model-00043-0.45584-0.85143-0.57648-0.78000.h5\n",
      "Epoch 44/50\n",
      "221/221 [==============================] - 68s 306ms/step - loss: 0.4441 - categorical_accuracy: 0.8560 - val_loss: 0.5522 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00044: saving model to model_init_2020-12-2511_49_09.401932/model-00044-0.44406-0.85596-0.55215-0.80000.h5\n",
      "Epoch 45/50\n",
      "221/221 [==============================] - 68s 308ms/step - loss: 0.4574 - categorical_accuracy: 0.8454 - val_loss: 0.5703 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00045: saving model to model_init_2020-12-2511_49_09.401932/model-00045-0.45738-0.84540-0.57032-0.82000.h5\n",
      "Epoch 46/50\n",
      "221/221 [==============================] - 68s 306ms/step - loss: 0.4468 - categorical_accuracy: 0.8544 - val_loss: 0.5887 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00046: saving model to model_init_2020-12-2511_49_09.401932/model-00046-0.44684-0.85445-0.58866-0.82000.h5\n",
      "Epoch 47/50\n",
      "221/221 [==============================] - 68s 307ms/step - loss: 0.5239 - categorical_accuracy: 0.8198 - val_loss: 0.6181 - val_categorical_accuracy: 0.7400\n",
      "\n",
      "Epoch 00047: saving model to model_init_2020-12-2511_49_09.401932/model-00047-0.52392-0.81976-0.61815-0.74000.h5\n",
      "Epoch 48/50\n",
      "221/221 [==============================] - 71s 321ms/step - loss: 0.5814 - categorical_accuracy: 0.8100 - val_loss: 0.5981 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00048: saving model to model_init_2020-12-2511_49_09.401932/model-00048-0.58136-0.80995-0.59813-0.80000.h5\n",
      "Epoch 49/50\n",
      "221/221 [==============================] - 72s 326ms/step - loss: 0.4711 - categorical_accuracy: 0.8303 - val_loss: 0.6748 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00049: saving model to model_init_2020-12-2511_49_09.401932/model-00049-0.47113-0.83032-0.67482-0.80000.h5\n",
      "Epoch 50/50\n",
      "221/221 [==============================] - 69s 314ms/step - loss: 0.4090 - categorical_accuracy: 0.8741 - val_loss: 0.6708 - val_categorical_accuracy: 0.8300\n",
      "\n",
      "Epoch 00050: saving model to model_init_2020-12-2511_49_09.401932/model-00050-0.40900-0.87406-0.67084-0.83000.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c50ef0e448>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If ablation shows good improvement run for entire train data\n",
    "\n",
    "train_generator = generator(train_path, train_doc, batch_size, augmentation=True)\n",
    "val_generator = generator(val_path, val_doc, batch_size)\n",
    "\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps,\n",
    "                    class_weight=None,\n",
    "                    workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/221 [============================>.] - ETA: 0s - loss: 0.3981 - categorical_accuracy: 0.8636"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:84: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 76s 343ms/step - loss: 0.4036 - categorical_accuracy: 0.8620 - val_loss: 0.5504 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00051: saving model to model_init_2020-12-2511_49_09.401932/model-00051-0.40363-0.86199-0.55045-0.85000.h5\n",
      "Epoch 52/100\n",
      "221/221 [==============================] - 69s 311ms/step - loss: 0.5272 - categorical_accuracy: 0.8190 - val_loss: 0.6973 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00052: saving model to model_init_2020-12-2511_49_09.401932/model-00052-0.52716-0.81900-0.69727-0.77000.h5\n",
      "Epoch 53/100\n",
      "221/221 [==============================] - 73s 330ms/step - loss: 0.4768 - categorical_accuracy: 0.8439 - val_loss: 0.5708 - val_categorical_accuracy: 0.7900\n",
      "\n",
      "Epoch 00053: saving model to model_init_2020-12-2511_49_09.401932/model-00053-0.47676-0.84389-0.57077-0.79000.h5\n",
      "Epoch 54/100\n",
      "221/221 [==============================] - 68s 307ms/step - loss: 0.4657 - categorical_accuracy: 0.8514 - val_loss: 0.5325 - val_categorical_accuracy: 0.8200\n",
      "\n",
      "Epoch 00054: saving model to model_init_2020-12-2511_49_09.401932/model-00054-0.46569-0.85143-0.53247-0.82000.h5\n",
      "Epoch 55/100\n",
      "221/221 [==============================] - 68s 308ms/step - loss: 0.4235 - categorical_accuracy: 0.8650 - val_loss: 0.6849 - val_categorical_accuracy: 0.7500\n",
      "\n",
      "Epoch 00055: saving model to model_init_2020-12-2511_49_09.401932/model-00055-0.42347-0.86501-0.68490-0.75000.h5\n",
      "Epoch 56/100\n",
      "221/221 [==============================] - 70s 315ms/step - loss: 0.3907 - categorical_accuracy: 0.8650 - val_loss: 0.4762 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00056: saving model to model_init_2020-12-2511_49_09.401932/model-00056-0.39071-0.86501-0.47623-0.80000.h5\n",
      "Epoch 57/100\n",
      "221/221 [==============================] - 70s 318ms/step - loss: 0.3493 - categorical_accuracy: 0.8876 - val_loss: 0.5577 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00057: saving model to model_init_2020-12-2511_49_09.401932/model-00057-0.34934-0.88763-0.55767-0.81000.h5\n",
      "Epoch 58/100\n",
      "221/221 [==============================] - 68s 307ms/step - loss: 0.3647 - categorical_accuracy: 0.8793 - val_loss: 0.5827 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00058: saving model to model_init_2020-12-2511_49_09.401932/model-00058-0.36472-0.87934-0.58266-0.78000.h5\n",
      "Epoch 59/100\n",
      "221/221 [==============================] - 68s 309ms/step - loss: 0.3914 - categorical_accuracy: 0.8733 - val_loss: 0.5274 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00059: saving model to model_init_2020-12-2511_49_09.401932/model-00059-0.39143-0.87330-0.52738-0.78000.h5\n",
      "Epoch 60/100\n",
      "221/221 [==============================] - 70s 316ms/step - loss: 0.3651 - categorical_accuracy: 0.8854 - val_loss: 0.5861 - val_categorical_accuracy: 0.8000\n",
      "\n",
      "Epoch 00060: saving model to model_init_2020-12-2511_49_09.401932/model-00060-0.36509-0.88537-0.58607-0.80000.h5\n",
      "Epoch 61/100\n",
      "221/221 [==============================] - 70s 316ms/step - loss: 0.3534 - categorical_accuracy: 0.8914 - val_loss: 0.5869 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00061: saving model to model_init_2020-12-2511_49_09.401932/model-00061-0.35340-0.89140-0.58688-0.77000.h5\n",
      "Epoch 62/100\n",
      "221/221 [==============================] - 71s 320ms/step - loss: 0.3448 - categorical_accuracy: 0.8967 - val_loss: 0.5414 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00062: saving model to model_init_2020-12-2511_49_09.401932/model-00062-0.34483-0.89668-0.54140-0.81000.h5\n",
      "Epoch 63/100\n",
      "221/221 [==============================] - 69s 311ms/step - loss: 0.3816 - categorical_accuracy: 0.8854 - val_loss: 0.5592 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00063: saving model to model_init_2020-12-2511_49_09.401932/model-00063-0.38162-0.88537-0.55922-0.85000.h5\n",
      "Epoch 64/100\n",
      "221/221 [==============================] - 70s 315ms/step - loss: 0.3714 - categorical_accuracy: 0.8846 - val_loss: 0.5430 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00064: saving model to model_init_2020-12-2511_49_09.401932/model-00064-0.37139-0.88462-0.54305-0.78000.h5\n",
      "Epoch 65/100\n",
      "221/221 [==============================] - 69s 311ms/step - loss: 0.3893 - categorical_accuracy: 0.8718 - val_loss: 0.6471 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00065: saving model to model_init_2020-12-2511_49_09.401932/model-00065-0.38932-0.87179-0.64712-0.77000.h5\n",
      "Epoch 66/100\n",
      "221/221 [==============================] - 69s 311ms/step - loss: 0.3374 - categorical_accuracy: 0.8959 - val_loss: 0.5394 - val_categorical_accuracy: 0.8400\n",
      "\n",
      "Epoch 00066: saving model to model_init_2020-12-2511_49_09.401932/model-00066-0.33743-0.89593-0.53939-0.84000.h5\n",
      "Epoch 67/100\n",
      "221/221 [==============================] - 68s 309ms/step - loss: 0.3439 - categorical_accuracy: 0.8869 - val_loss: 0.5421 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00067: saving model to model_init_2020-12-2511_49_09.401932/model-00067-0.34393-0.88688-0.54210-0.81000.h5\n",
      "Epoch 68/100\n",
      "221/221 [==============================] - 70s 315ms/step - loss: 0.3992 - categorical_accuracy: 0.8778 - val_loss: 0.5977 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00068: saving model to model_init_2020-12-2511_49_09.401932/model-00068-0.39919-0.87783-0.59765-0.77000.h5\n",
      "Epoch 69/100\n",
      "221/221 [==============================] - 68s 306ms/step - loss: 0.3415 - categorical_accuracy: 0.8929 - val_loss: 0.5476 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00069: saving model to model_init_2020-12-2511_49_09.401932/model-00069-0.34153-0.89291-0.54757-0.78000.h5\n",
      "Epoch 70/100\n",
      "221/221 [==============================] - 71s 320ms/step - loss: 0.2884 - categorical_accuracy: 0.9087 - val_loss: 0.6169 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00070: saving model to model_init_2020-12-2511_49_09.401932/model-00070-0.28837-0.90875-0.61694-0.77000.h5\n",
      "Epoch 71/100\n",
      "221/221 [==============================] - 69s 310ms/step - loss: 0.2731 - categorical_accuracy: 0.9140 - val_loss: 0.5854 - val_categorical_accuracy: 0.8500\n",
      "\n",
      "Epoch 00071: saving model to model_init_2020-12-2511_49_09.401932/model-00071-0.27305-0.91403-0.58537-0.85000.h5\n",
      "Epoch 72/100\n",
      "221/221 [==============================] - 69s 312ms/step - loss: 0.3618 - categorical_accuracy: 0.8929 - val_loss: 0.7510 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00072: saving model to model_init_2020-12-2511_49_09.401932/model-00072-0.36175-0.89291-0.75098-0.77000.h5\n",
      "Epoch 73/100\n",
      "221/221 [==============================] - 69s 310ms/step - loss: 0.3817 - categorical_accuracy: 0.8808 - val_loss: 0.6473 - val_categorical_accuracy: 0.7700\n",
      "\n",
      "Epoch 00073: saving model to model_init_2020-12-2511_49_09.401932/model-00073-0.38166-0.88084-0.64729-0.77000.h5\n",
      "Epoch 74/100\n",
      "221/221 [==============================] - 70s 318ms/step - loss: 0.3824 - categorical_accuracy: 0.8869 - val_loss: 0.5421 - val_categorical_accuracy: 0.7800\n",
      "\n",
      "Epoch 00074: saving model to model_init_2020-12-2511_49_09.401932/model-00074-0.38238-0.88688-0.54209-0.78000.h5\n",
      "Epoch 75/100\n",
      "221/221 [==============================] - 69s 312ms/step - loss: 0.3737 - categorical_accuracy: 0.8846 - val_loss: 0.5747 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00075: saving model to model_init_2020-12-2511_49_09.401932/model-00075-0.37365-0.88462-0.57470-0.81000.h5\n",
      "Epoch 76/100\n",
      "221/221 [==============================] - 68s 306ms/step - loss: 0.4251 - categorical_accuracy: 0.8688 - val_loss: 1.6807 - val_categorical_accuracy: 0.5900\n",
      "\n",
      "Epoch 00076: saving model to model_init_2020-12-2511_49_09.401932/model-00076-0.42509-0.86878-1.68068-0.59000.h5\n",
      "Epoch 00076: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5147d0d08>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=100, verbose=1, \n",
    "                    callbacks=callbacks_list, validation_data=val_generator, \n",
    "                    validation_steps=validation_steps,\n",
    "                    class_weight=None,\n",
    "                    workers=1, initial_epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'model_init_2020-12-2511_49_09.401932/model-00063-0.38162-0.88537-0.55922-0.85000.h5'\n",
    "\n",
    "import keras\n",
    "loaded_model2 = keras.models.load_model(model_path) #Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator_generator = generator(val_path, val_doc, 100) #Load Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/val ; batch size = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "C:\\Users\\adi00\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data, labels = next(validator_generator) #Assign Data and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5561961883306503, 0.85]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model2.evaluate(data, labels) #Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
